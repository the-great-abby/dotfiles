#!/bin/bash
# GTD Brain - Bidirectional Obsidian Sync
# Detects changes in Obsidian, merges conflicts, syncs metadata

# Source common environment
COMMON_ENV="$HOME/code/dotfiles/zsh/common_env.sh"
if [[ ! -f "$COMMON_ENV" && -f "$HOME/code/personal/dotfiles/zsh/common_env.sh" ]]; then
  COMMON_ENV="$HOME/code/personal/dotfiles/zsh/common_env.sh"
fi
if [[ -f "$COMMON_ENV" ]]; then
  source "$COMMON_ENV"
fi

# Load GTD config
GTD_CONFIG_FILE="$HOME/.gtd_config"
if [[ -f "$HOME/code/dotfiles/zsh/.gtd_config" ]]; then
  GTD_CONFIG_FILE="$HOME/code/dotfiles/zsh/.gtd_config"
elif [[ -f "$HOME/code/personal/dotfiles/zsh/.gtd_config" ]]; then
  GTD_CONFIG_FILE="$HOME/code/personal/dotfiles/zsh/.gtd_config"
fi

if [[ -f "$GTD_CONFIG_FILE" ]]; then
  source "$GTD_CONFIG_FILE"
fi

# Default values
SECOND_BRAIN="${SECOND_BRAIN:-$HOME/Documents/obsidian/Second Brain}"
DAILY_LOG_DIR="${DAILY_LOG_DIR:-$HOME/Documents/daily_logs}"
DAILY_NOTES_DIR="${SECOND_BRAIN}/Daily Notes"
SYNC_METADATA_FILE="$HOME/.gtd_brain_sync_metadata.json"

# Colors
CYAN='\033[0;36m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
RED='\033[0;31m'
BOLD='\033[1m'
NC='\033[0m'

# Get date command
get_date_cmd() {
  if [[ -x "/usr/bin/date" ]]; then
    echo "/usr/bin/date"
  elif [[ -x "/bin/date" ]]; then
    echo "/bin/date"
  else
    echo "date"
  fi
}

DATE_CMD=$(get_date_cmd)

# Initialize sync metadata
init_sync_metadata() {
  if [[ ! -f "$SYNC_METADATA_FILE" ]]; then
    cat > "$SYNC_METADATA_FILE" <<EOF
{
  "created": "$($DATE_CMD -Iseconds)",
  "last_sync": null,
  "file_timestamps": {},
  "conflicts": [],
  "metadata_changes": []
}
EOF
    chmod 644 "$SYNC_METADATA_FILE" 2>/dev/null || true
  fi
}

# Get file metadata (modification time, content hash)
get_file_metadata() {
  local file_path="$1"
  
  if [[ ! -f "$file_path" ]]; then
    echo "null"
    return
  fi
  
  python3 <<EOF
import os
import hashlib
import json
from pathlib import Path

file_path = "$file_path"
if not os.path.exists(file_path):
    print("null")
else:
    stat = os.stat(file_path)
    mtime = stat.st_mtime
    
    # Calculate content hash
    with open(file_path, 'rb') as f:
        content_hash = hashlib.md5(f.read()).hexdigest()
    
    # Extract metadata from markdown frontmatter if present
    metadata = {}
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
            if content.startswith('---'):
                # Try to extract frontmatter
                parts = content.split('---', 2)
                if len(parts) >= 3:
                    frontmatter = parts[1]
                    # Simple YAML-like parsing for common fields
                    for line in frontmatter.split('\n'):
                        if ':' in line:
                            key, value = line.split(':', 1)
                            key = key.strip()
                            value = value.strip().strip('"').strip("'")
                            metadata[key] = value
    except:
        pass
    
    result = {
        "mtime": mtime,
        "hash": content_hash,
        "size": stat.st_size,
        "metadata": metadata
    }
    print(json.dumps(result))
EOF
}

# Detect changes in Obsidian (Daily Notes, Zettelkasten, etc.)
detect_obsidian_changes() {
  local days_back="${1:-7}"
  local quiet="${2:-false}"
  
  if [[ "$quiet" != "true" ]]; then
    echo "ğŸ” Detecting changes in Obsidian..." >&2
    echo "" >&2
  fi
  
  init_sync_metadata
  
  python3 <<EOF
import json
import os
from pathlib import Path
from datetime import datetime, timedelta
import hashlib

second_brain = Path("$SECOND_BRAIN")
daily_notes_dir = Path("$DAILY_NOTES_DIR")
daily_log_dir = Path("$DAILY_LOG_DIR")
sync_metadata_file = Path("$SYNC_METADATA_FILE")
days_back = int("$days_back")

# Load sync metadata
if sync_metadata_file.exists():
    with open(sync_metadata_file) as f:
        metadata = json.load(f)
else:
    metadata = {"file_timestamps": {}}

file_timestamps = metadata.get("file_timestamps", {})
changes = []
new_files = []
missing_local_logs = []

# Check Daily Notes directory - this is the key for bidirectional sync
if daily_notes_dir.exists():
    cutoff_time = datetime.now() - timedelta(days=days_back)
    
    for note_file in daily_notes_dir.glob("*.md"):
        file_path = str(note_file)
        rel_path = str(note_file.relative_to(second_brain))
        date = note_file.stem  # filename without extension
        
        stat = note_file.stat()
        mtime = datetime.fromtimestamp(stat.st_mtime)
        
        # Check if corresponding local log exists
        local_log = daily_log_dir / f"{date}.md"
        has_local_log = local_log.exists()
        
        # Check if file is new or modified
        if rel_path in file_timestamps:
            old_mtime = file_timestamps[rel_path].get("mtime", 0)
            if stat.st_mtime > old_mtime:
                # File was modified in Obsidian
                change_info = {
                    "path": rel_path,
                    "full_path": file_path,
                    "type": "modified",
                    "last_modified": mtime.isoformat(),
                    "mtime": stat.st_mtime,
                    "has_local_log": has_local_log,
                    "date": date
                }
                changes.append(change_info)
                
                # If modified in Obsidian but no local log, this needs pulling
                if not has_local_log:
                    missing_local_logs.append(change_info)
        else:
            # New file (or not tracked yet)
            if mtime > cutoff_time or not has_local_log:
                new_info = {
                    "path": rel_path,
                    "full_path": file_path,
                    "type": "new",
                    "created": mtime.isoformat(),
                    "mtime": stat.st_mtime,
                    "has_local_log": has_local_log,
                    "date": date
                }
                new_files.append(new_info)
                
                # If no local log, needs to be pulled
                if not has_local_log:
                    missing_local_logs.append(new_info)

# Check Zettelkasten directory
zettelkasten_dir = second_brain / "Zettelkasten"
if zettelkasten_dir.exists():
    for note_file in zettelkasten_dir.rglob("*.md"):
        file_path = str(note_file)
        rel_path = str(note_file.relative_to(second_brain))
        
        stat = note_file.stat()
        mtime = datetime.fromtimestamp(stat.st_mtime)
        
        if rel_path in file_timestamps:
            old_mtime = file_timestamps[rel_path].get("mtime", 0)
            if stat.st_mtime > old_mtime:
                changes.append({
                    "path": rel_path,
                    "full_path": file_path,
                    "type": "modified",
                    "last_modified": mtime.isoformat(),
                    "mtime": stat.st_mtime,
                    "has_local_log": False  # Zettelkasten notes don't have local logs
                })
        else:
            # New file (not tracked yet) - include if recent OR if never tracked
            # This catches notes created in Obsidian that haven't been tracked
            # New file (not tracked yet) - include it regardless of age
            # This catches files created in Obsidian that haven't been tracked
            new_files.append({
                "path": rel_path,
                "full_path": file_path,
                "type": "new",
                "created": mtime.isoformat(),
                "mtime": stat.st_mtime,
                "has_local_log": False
            })

# Check MOCs directory
mocs_dir = second_brain / "MOCs"
if mocs_dir.exists():
    for moc_file in mocs_dir.rglob("*.md"):
        file_path = str(moc_file)
        rel_path = str(moc_file.relative_to(second_brain))
        
        stat = moc_file.stat()
        mtime = datetime.fromtimestamp(stat.st_mtime)
        
        if rel_path in file_timestamps:
            old_mtime = file_timestamps[rel_path].get("mtime", 0)
            if stat.st_mtime > old_mtime:
                changes.append({
                    "path": rel_path,
                    "full_path": file_path,
                    "type": "modified",
                    "last_modified": mtime.isoformat(),
                    "mtime": stat.st_mtime,
                    "has_local_log": False
                })
        else:
            # New file (not tracked yet) - include it regardless of age
            # This catches MOCs created in Obsidian that haven't been tracked
            new_files.append({
                "path": rel_path,
                "full_path": file_path,
                "type": "new",
                "created": mtime.isoformat(),
                "mtime": stat.st_mtime,
                "has_local_log": False
            })

# Check Resources, Projects, Areas directories
for para_dir in ["Resources", "Projects", "Areas"]:
    para_path = second_brain / para_dir
    if para_path.exists():
        for note_file in para_path.rglob("*.md"):
            file_path = str(note_file)
            rel_path = str(note_file.relative_to(second_brain))
            
            stat = note_file.stat()
            mtime = datetime.fromtimestamp(stat.st_mtime)
            
            if rel_path in file_timestamps:
                old_mtime = file_timestamps[rel_path].get("mtime", 0)
                if stat.st_mtime > old_mtime:
                    changes.append({
                        "path": rel_path,
                        "full_path": file_path,
                        "type": "modified",
                        "last_modified": mtime.isoformat(),
                        "mtime": stat.st_mtime,
                        "has_local_log": False
                    })
            else:
                if mtime > cutoff_time:
                    new_files.append({
                        "path": rel_path,
                        "full_path": file_path,
                        "type": "new",
                        "created": mtime.isoformat(),
                        "mtime": stat.st_mtime,
                        "has_local_log": False
                    })

result = {
    "changes": changes,
    "new_files": new_files,
    "missing_local_logs": missing_local_logs,
    "total_changes": len(changes) + len(new_files),
    "total_pending_pulls": len(missing_local_logs)
}

print(json.dumps(result, indent=2))
EOF
}

# Merge conflicts intelligently
merge_conflict() {
  local local_file="$1"
  local obsidian_file="$2"
  local conflict_type="${3:-content}"
  local quiet="${4:-false}"
  
  # Send conflict message to stderr (won't be captured) unless quiet mode
  if [[ "$quiet" != "true" ]]; then
    echo "âš ï¸  Conflict detected: $conflict_type" >&2
    echo "   Local: $local_file" >&2
    echo "   Obsidian: $obsidian_file" >&2
    echo "" >&2
  fi
  
  python3 <<EOF
import json
from pathlib import Path
from datetime import datetime

local_file = Path("$local_file")
obsidian_file = Path("$obsidian_file")
conflict_type = "$conflict_type"

# Read both files
local_content = local_file.read_text() if local_file.exists() else ""
obsidian_content = obsidian_file.read_text() if obsidian_file.exists() else ""

# Simple merge strategy: combine unique entries
if conflict_type == "daily_log":
    import re
    # For daily logs, merge entries by time
    local_lines = local_content.split('\n')
    obsidian_lines = obsidian_content.split('\n')
    
    # Extract time-stamped entries from both (format: HH:MM - entry)
    local_entries = {}
    obsidian_entries = {}
    
    # Extract from local file
    for line in local_lines:
        line = line.strip()
        # Match time-stamped entries: HH:MM - entry
        match = re.match(r'^(\d{2}:\d{2})\s+-\s+(.+)$', line)
        if match:
            time_key = match.group(1)  # HH:MM
            entry_text = match.group(2)
            local_entries[time_key] = f"{time_key} - {entry_text}"
    
    # Extract from Obsidian file (might be in code blocks or sections)
    for line in obsidian_lines:
        line = line.strip()
        # Match time-stamped entries: HH:MM - entry
        match = re.match(r'^(\d{2}:\d{2})\s+-\s+(.+)$', line)
        if match:
            time_key = match.group(1)  # HH:MM
            entry_text = match.group(2)
            obsidian_entries[time_key] = f"{time_key} - {entry_text}"
    
    # Merge: Obsidian entries take precedence for same time, but add unique ones
    merged_entries = dict(local_entries)
    for time_key, entry in obsidian_entries.items():
        if time_key not in merged_entries or obsidian_file.stat().st_mtime > local_file.stat().st_mtime:
            merged_entries[time_key] = entry
    
    # Get header (first line, usually the date)
    header = ""
    if local_content:
        first_line = local_content.split('\n')[0].strip()
        if first_line and not re.match(r'^\d{2}:\d{2}', first_line):
            header = first_line
    elif obsidian_content:
        first_line = obsidian_content.split('\n')[0].strip()
        if first_line and not re.match(r'^\d{2}:\d{2}', first_line):
            header = first_line
    
    # Reconstruct content with sorted entries
    sorted_entries = sorted(merged_entries.values())
    if header:
        merged_content = header + "\n\n" + "\n".join(sorted_entries)
    else:
        merged_content = "\n".join(sorted_entries)
    
    print(merged_content)
else:
    # For other files, use newer version by default, but flag for manual review
    if obsidian_file.exists() and local_file.exists():
        if obsidian_file.stat().st_mtime > local_file.stat().st_mtime:
            print(obsidian_content)
            print("\n<!-- MERGED: Obsidian version used (newer timestamp) -->")
        else:
            print(local_content)
            print("\n<!-- MERGED: Local version used (newer timestamp) -->")
    elif obsidian_file.exists():
        print(obsidian_content)
    else:
        print(local_content)
EOF
}

# Sync metadata (tags, links, status)
sync_metadata() {
  local file_path="$1"
  
  python3 <<EOF
import re
from pathlib import Path

file_path = Path("$file_path")
if not file_path.exists():
    print("{}")
    exit(0)

content = file_path.read_text()

# Extract tags
tags = re.findall(r'#([a-zA-Z0-9_-]+)', content)

# Extract links
links = re.findall(r'\[\[([^\]]+)\]\]', content)

# Extract frontmatter metadata
metadata = {}
if content.startswith('---'):
    parts = content.split('---', 2)
    if len(parts) >= 3:
        frontmatter = parts[1]
        for line in frontmatter.split('\n'):
            if ':' in line:
                key, value = line.split(':', 1)
                key = key.strip()
                value = value.strip().strip('"').strip("'")
                metadata[key] = value

import json
result = {
    "tags": list(set(tags)),
    "links": list(set(links)),
    "frontmatter": metadata
}
print(json.dumps(result, indent=2))
EOF
}

# Bidirectional sync for daily logs
sync_daily_logs_bidirectional() {
  echo ""
  echo -e "${BOLD}${CYAN}ğŸ”„ Bidirectional Daily Log Sync${NC}"
  echo -e "${CYAN}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${NC}"
  echo ""
  
  mkdir -p "$DAILY_LOG_DIR"
  mkdir -p "$DAILY_NOTES_DIR"
  
  local synced=0
  local conflicts=0
  local pulled=0
  
  # 1. Check for Daily Notes that don't have local logs OR were modified in Obsidian
  find "$DAILY_NOTES_DIR" -type f -name "*.md" -maxdepth 1 | while read -r note_file; do
    local date=$(basename "$note_file" .md)
    local log_file="${DAILY_LOG_DIR}/${date}.md"
    
    # Extract daily log content from Daily Note
    if [[ -f "$note_file" ]]; then
      # Check if note was modified in Obsidian
      local note_mtime=$(stat -f %m "$note_file" 2>/dev/null || stat -c %Y "$note_file" 2>/dev/null)
      
      if [[ ! -f "$log_file" ]]; then
        # No local log - extract from Daily Note
        echo "ğŸ“¥ Pulling log from Obsidian (no local file): $date"
        
        # Extract log content from Daily Note - use separate Python script
        extract_script=""
        if [[ -f "$HOME/code/dotfiles/bin/gtd-extract-log-from-note.py" ]]; then
          extract_script="$HOME/code/dotfiles/bin/gtd-extract-log-from-note.py"
        elif [[ -f "$(dirname "$0")/gtd-extract-log-from-note.py" ]]; then
          extract_script="$(dirname "$0")/gtd-extract-log-from-note.py"
        fi
        
        if [[ -n "$extract_script" ]]; then
          extracted=$(python3 "$extract_script" "$note_file" "$log_file" "$date" 2>&1)
        else
          # Fallback: create empty file
          mkdir -p "$(dirname "$log_file")"
          echo "$date" > "$log_file"
          echo "" >> "$log_file"
          echo "<!-- Log extracted from Obsidian Daily Note: $date -->" >> "$log_file"
          echo "<!-- Note: This log was created from Obsidian. Add entries below. -->" >> "$log_file"
          echo "" >> "$log_file"
          extracted="EMPTY: $log_file"
        fi
        
        if echo "$extracted" | grep -q "SUCCESS:"; then
          echo "âœ“ Extracted and pulled log from Obsidian Daily Note"
          echo "$extracted" | grep "SUCCESS:" | sed 's/SUCCESS: /  Created: /'
        elif echo "$extracted" | grep -q "EMPTY:"; then
          echo "âš ï¸  Could not extract log entries from Daily Note, created empty log file"
        fi
        ((pulled++))
      else
        # Both exist - check which is newer and merge if needed
        local log_mtime=$(stat -f %m "$log_file" 2>/dev/null || stat -c %Y "$log_file" 2>/dev/null)
        
        # Calculate time difference (in seconds)
        local time_diff=$((note_mtime - log_mtime))
        
        # Only treat as conflict if:
        # 1. Obsidian is significantly newer (more than 5 seconds) - likely edited in Obsidian
        # 2. OR local is significantly newer - needs syncing
        # This avoids false conflicts from sync operations that happen within seconds
        
        if [[ $time_diff -gt 5 ]]; then
          # Obsidian is significantly newer - likely edited in Obsidian, need to merge
          echo "ğŸ”„ Merging changes from Obsidian: $date"
          # Use quiet mode to suppress conflict message (we already know it's a merge)
          merged_content=$(merge_conflict "$log_file" "$note_file" "daily_log" "true" 2>/dev/null)
          if [[ -n "$merged_content" ]]; then
            echo "$merged_content" > "$log_file"
            echo "  âœ“ Merged log entries successfully"
            ((synced++))
          else
            echo "  âš ï¸  Merge failed, keeping local version"
          fi
        elif [[ $time_diff -lt -5 ]]; then
          # Local is significantly newer - sync to Obsidian
          echo "ğŸ“¤ Syncing to Obsidian: $date"
          # Update Daily Note (but preserve structure)
          if command -v gtd-brain-sync-daily-logs &>/dev/null; then
            gtd-brain-sync-daily-logs "$date" >/dev/null 2>&1
          fi
          ((synced++))
        else
          # Files are within 5 seconds of each other - likely just synced, no action needed
          # Silently skip to avoid false conflict messages
          :
        fi
      fi
    fi
  done
  
  # 2. Check for local logs that aren't in Daily Notes
  find "$DAILY_LOG_DIR" -type f -name "*.md" -maxdepth 1 | while read -r log_file; do
    local date=$(basename "$log_file" .md)
    local note_file="${DAILY_NOTES_DIR}/${date}.md"
    
    if [[ ! -f "$note_file" ]]; then
      echo "ğŸ“¤ Creating Daily Note from log: $date"
      if command -v gtd-brain-sync-daily-logs &>/dev/null; then
        gtd-brain-sync-daily-logs "$date" >/dev/null 2>&1
      fi
      ((synced++))
    fi
  done
  
  echo ""
  echo "âœ“ Sync complete: ${synced} synced, ${pulled} pulled from Obsidian"
  echo ""
}

# Sync MOCs, Zettelkasten, and other Second Brain files
# Updates metadata tracking for files that were created/modified in Obsidian
sync_second_brain_files() {
  local changes_json="$1"
  
  echo ""
  echo -e "${BOLD}${CYAN}ğŸ“š Syncing Second Brain Files (MOCs, Zettelkasten, etc.)${NC}"
  echo -e "${CYAN}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${NC}"
  echo ""
  
  # Even if changes_json is empty, we still want to scan for untracked files
  # So we'll pass an empty dict if needed
  
  python3 <<EOF
import json
from pathlib import Path
from datetime import datetime

changes_json = '''$changes_json'''
second_brain = Path("$SECOND_BRAIN")
sync_metadata_file = Path("$SYNC_METADATA_FILE")

try:
    # Parse changes_json, default to empty dict if invalid/empty
    if changes_json and changes_json.strip() and changes_json != "{}":
        changes_data = json.loads(changes_json)
        new_files = changes_data.get("new_files", [])
        changes = changes_data.get("changes", [])
    else:
        new_files = []
        changes = []
    
    # Load metadata
    if sync_metadata_file.exists():
        with open(sync_metadata_file) as f:
            metadata = json.load(f)
    else:
        metadata = {"file_timestamps": {}}
    
    file_timestamps = metadata.get("file_timestamps", {})
    files_processed = 0
    mocs_tracked = 0
    zettelkasten_tracked = 0
    resources_tracked = 0
    moc_list = []
    
    # Process new files (MOCs, Zettelkasten, etc.)
    for file_info in new_files:
        rel_path = file_info.get("path", "")
        if not rel_path:
            continue
        
        # Skip daily notes (handled separately)
        if "Daily Notes" in rel_path:
            continue
        
        full_path = second_brain / rel_path
        if full_path.exists():
            stat = full_path.stat()
            file_timestamps[rel_path] = {
                "mtime": stat.st_mtime,
                "size": stat.st_size,
                "last_synced": datetime.now().isoformat(),
                "type": file_info.get("type", "new")
            }
            files_processed += 1
            
            # Count by type and collect MOC names
            if "MOCs" in rel_path or "MOC" in rel_path:
                mocs_tracked += 1
                moc_name = Path(rel_path).name.replace(".md", "").replace("MOC - ", "")
                moc_list.append(moc_name)
            elif "Zettelkasten" in rel_path:
                zettelkasten_tracked += 1
            elif "Resources" in rel_path:
                resources_tracked += 1
    
    # Process modified files
    for change_info in changes:
        rel_path = change_info.get("path", "")
        if not rel_path:
            continue
        
        # Skip daily notes (handled separately)
        if "Daily Notes" in rel_path:
            continue
        
        full_path = second_brain / rel_path
        if full_path.exists():
            stat = full_path.stat()
            file_timestamps[rel_path] = {
                "mtime": stat.st_mtime,
                "size": stat.st_size,
                "last_synced": datetime.now().isoformat(),
                "type": change_info.get("type", "modified")
            }
            files_processed += 1
            
            # Count by type and collect MOC names
            if "MOCs" in rel_path or "MOC" in rel_path:
                mocs_tracked += 1
                moc_name = Path(rel_path).name.replace(".md", "").replace("MOC - ", "")
                if moc_name not in moc_list:
                    moc_list.append(moc_name)
            elif "Zettelkasten" in rel_path:
                zettelkasten_tracked += 1
            elif "Resources" in rel_path:
                resources_tracked += 1
    
    # Save updated metadata
    metadata["file_timestamps"] = file_timestamps
    with open(sync_metadata_file, 'w') as f:
        json.dump(metadata, f, indent=2)
    
    # Also check for MOCs that exist but aren't in metadata (catch-all)
    moc_dir = second_brain / "MOCs"
    if moc_dir.exists():
        for moc_file in moc_dir.glob("*.md"):
            rel_path = str(moc_file.relative_to(second_brain))
            if rel_path not in file_timestamps:
                # MOC exists but not tracked - add it
                stat = moc_file.stat()
                file_timestamps[rel_path] = {
                    "mtime": stat.st_mtime,
                    "size": stat.st_size,
                    "last_synced": datetime.now().isoformat(),
                    "type": "existing"
                }
                files_processed += 1
                mocs_tracked += 1
                moc_name = moc_file.name.replace(".md", "").replace("MOC - ", "")
                if moc_name not in moc_list:
                    moc_list.append(moc_name)
    
    if files_processed > 0:
        print(f"âœ“ Acknowledged {files_processed} Second Brain file(s) from Obsidian:")
        print("")
        if mocs_tracked > 0:
            print(f"   ğŸ“š MOCs: {mocs_tracked} file(s)")
            for moc_name in sorted(set(moc_list))[:10]:  # Show up to 10, sorted
                print(f"      â€¢ {moc_name}")
            if len(moc_list) > 10:
                print(f"      ... and {len(moc_list) - 10} more")
        if zettelkasten_tracked > 0:
            print(f"   ğŸ”— Zettelkasten: {zettelkasten_tracked} note(s)")
        if resources_tracked > 0:
            print(f"   ğŸ“ Resources: {resources_tracked} file(s)")
        if files_processed - mocs_tracked - zettelkasten_tracked - resources_tracked > 0:
            print(f"   ğŸ“„ Other: {files_processed - mocs_tracked - zettelkasten_tracked - resources_tracked} file(s)")
        print("")
        print("âœ… Verification:")
        if moc_dir.exists():
            local_moc_count = len(list(moc_dir.glob("*.md")))
            tracked_moc_count = len([p for p in file_timestamps.keys() if "MOCs" in p or "MOC" in p])
            print(f"   ğŸ“š Local MOCs directory: {moc_dir}")
            print(f"   ğŸ“Š Total MOCs in local system: {local_moc_count}")
            print(f"   ğŸ“‹ MOCs tracked in sync metadata: {tracked_moc_count}")
        print("")
        print("ğŸ’¡ These files are already in your local Second Brain directory")
        print("   (synced automatically via Obsidian iCloud/Dropbox)")
        print("ğŸ’¡ Sync metadata has been updated to track them for future syncs")
        print("")
    else:
        # Still show MOC count even if nothing new
        if moc_dir.exists():
            local_moc_count = len(list(moc_dir.glob("*.md")))
            tracked_moc_count = len([p for p in file_timestamps.keys() if "MOCs" in p or "MOC" in p])
            if local_moc_count > 0:
                print(f"âœ“ Found {local_moc_count} MOC(s) in local system")
                print(f"  {tracked_moc_count} already tracked in sync metadata")
                print("")
        else:
            print("âœ“ No new Second Brain files detected from Obsidian")
            print("")
except Exception as e:
    # Silently fail - this is just metadata tracking
    pass
EOF
}

# Sync all Second Brain notes (detect changes)
sync_all_notes() {
  local force="${1:-false}"
  
  echo ""
  echo -e "${BOLD}${CYAN}ğŸ”„ Full Bidirectional Sync${NC}"
  echo -e "${CYAN}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${NC}"
  echo ""
  
  # Detect changes (quiet mode to avoid breaking JSON parsing)
  local changes_json=$(detect_obsidian_changes 7 "true")
  
  python3 <<EOF
import json

changes_json = '''$changes_json'''
try:
    changes_data = json.loads(changes_json)
    
    changes = changes_data.get("changes", [])
    new_files = changes_data.get("new_files", [])
    total = changes_data.get("total_changes", 0)
    
    missing_logs = changes_data.get("missing_local_logs", [])
    pending_pulls = changes_data.get("total_pending_pulls", 0)
    
    if total == 0:
        print("âœ“ No changes detected in Obsidian")
        print("")
    else:
        print(f"ğŸ“Š Detected {total} change(s) in Obsidian:")
        print("")
        
        if pending_pulls > 0:
            print(f"âš ï¸  {pending_pulls} Daily Note(s) in Obsidian without local log files:")
            for file in missing_logs[:10]:
                date = file.get('date', 'unknown')
                print(f"   ğŸ“… {date} - Needs to be pulled from Obsidian")
            print("")
            print(f"ğŸ’¡ These will be automatically pulled when you run sync-daily-logs")
            print("")
        
        if new_files:
            print("ğŸ“„ New Files:")
            for file in new_files[:10]:  # Show first 10
                print(f"   + {file['path']}")
            print("")
        
        if changes:
            print("ğŸ“ Modified Files:")
            for change in changes[:10]:  # Show first 10
                print(f"   âœï¸  {change['path']}")
            print("")
        
        print(f"ğŸ’¡ MOCs, Zettelkasten, and other files will be tracked in sync metadata")
        print(f"ğŸ’¡ Run 'gtd-brain-bidirectional-sync sync-daily-logs' to sync daily logs")
        print(f"ğŸ’¡ Or use 'gtd-brain-bidirectional-sync resolve' to handle changes manually")
        print("")
except json.JSONDecodeError as e:
    print(f"âš ï¸  Error parsing changes data: {e}")
    print(f"   Raw output: {changes_json[:200]}...")
    print("")
except Exception as e:
    print(f"âš ï¸  Error detecting changes: {e}")
    print("")
EOF
  
  # Store changes_json for later use (even if empty, we still want to scan for untracked files)
  local stored_changes_json="$changes_json"
  
  # Show detected changes
  echo "ğŸ” Detecting changes in Obsidian..."
  echo ""
  
  # Sync daily logs
  sync_daily_logs_bidirectional
  
  # Sync MOCs, Zettelkasten, and other Second Brain files
  # (Update metadata tracking even though files are already synced via Obsidian)
  # Pass changes_json even if empty - function will scan for untracked files
  sync_second_brain_files "${stored_changes_json:-{}}"
  
  # Update sync metadata
  python3 <<EOF
import json
from pathlib import Path
from datetime import datetime

sync_metadata_file = Path("$SYNC_METADATA_FILE")
if sync_metadata_file.exists():
    with open(sync_metadata_file) as f:
        metadata = json.load(f)
else:
    metadata = {"file_timestamps": {}}

metadata["last_sync"] = datetime.now().isoformat()

# Update file timestamps for synced files
from pathlib import Path
import os

second_brain = Path("$SECOND_BRAIN")
daily_notes_dir = Path("$DAILY_NOTES_DIR")

if daily_notes_dir.exists():
    for note_file in daily_notes_dir.glob("*.md"):
        rel_path = str(note_file.relative_to(second_brain))
        stat = note_file.stat()
        
        metadata["file_timestamps"][rel_path] = {
            "mtime": stat.st_mtime,
            "size": stat.st_size,
            "last_synced": datetime.now().isoformat()
        }

with open(sync_metadata_file, 'w') as f:
    json.dump(metadata, f, indent=2)
EOF
}

# Show sync status
show_status() {
  echo ""
  echo -e "${BOLD}${CYAN}ğŸ“Š Bidirectional Sync Status${NC}"
  echo -e "${CYAN}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${NC}"
  echo ""
  
  # Check for changes
  local changes_json=$(detect_obsidian_changes 7)
  
  python3 <<EOF
import json
from pathlib import Path
from datetime import datetime, timedelta

changes_json = '''$changes_json'''
sync_metadata_file = Path("$SYNC_METADATA_FILE")
second_brain = Path("$SECOND_BRAIN")
daily_log_dir = Path("$DAILY_LOG_DIR")
daily_notes_dir = Path("$DAILY_NOTES_DIR")

# Load metadata
if sync_metadata_file.exists():
    with open(sync_metadata_file) as f:
        metadata = json.load(f)
    last_sync = metadata.get("last_sync")
    if last_sync:
        last_sync_dt = datetime.fromisoformat(last_sync.replace('Z', '+00:00').replace('+00:00', ''))
        print(f"ğŸ• Last Sync: {last_sync_dt.strftime('%Y-%m-%d %H:%M:%S')}")
        print("")
else:
    print("ğŸ• Last Sync: Never")
    print("")

# Count files
local_logs = len(list(daily_log_dir.glob("*.md"))) if daily_log_dir.exists() else 0
daily_notes = len(list(daily_notes_dir.glob("*.md"))) if daily_notes_dir.exists() else 0

print(f"ğŸ“ Local Daily Logs: {local_logs}")
print(f"ğŸ“± Daily Notes in Obsidian: {daily_notes}")
print("")

# Show changes
try:
    changes_data = json.loads(changes_json)
    changes = changes_data.get("changes", [])
    new_files = changes_data.get("new_files", [])
    total = changes_data.get("total_changes", 0)
    
    if total > 0:
        print(f"âš ï¸  {total} change(s) detected in Obsidian (last 7 days)")
        print(f"   - {len(new_files)} new file(s)")
        print(f"   - {len(changes)} modified file(s)")
        print("")
        print("ğŸ’¡ Run 'gtd-brain-bidirectional-sync sync' to sync changes")
    else:
        print("âœ… No pending changes detected")
        print("")
except:
    pass

# Check for conflicts
if sync_metadata_file.exists():
    with open(sync_metadata_file) as f:
        metadata = json.load(f)
    conflicts = metadata.get("conflicts", [])
    if conflicts:
        print(f"âš ï¸  {len(conflicts)} conflict(s) need resolution")
        for conflict in conflicts[:5]:
            print(f"   - {conflict.get('path', 'unknown')}")
    else:
        print("âœ… No conflicts")
EOF
  
  echo ""
}

# Resolve conflicts
resolve_conflicts() {
  echo ""
  echo -e "${BOLD}${CYAN}ğŸ”§ Resolve Conflicts${NC}"
  echo -e "${CYAN}â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”${NC}"
  echo ""
  
  python3 <<EOF
import json
from pathlib import Path

sync_metadata_file = Path("$SYNC_METADATA_FILE")
if not sync_metadata_file.exists():
    print("âœ“ No conflicts to resolve")
    exit(0)

with open(sync_metadata_file) as f:
    metadata = json.load(f)

conflicts = metadata.get("conflicts", [])
if not conflicts:
    print("âœ“ No conflicts to resolve")
else:
    print(f"Found {len(conflicts)} conflict(s):")
    print("")
    for i, conflict in enumerate(conflicts, 1):
        print(f"{i}. {conflict.get('path', 'unknown')}")
        print(f"   Type: {conflict.get('type', 'unknown')}")
        print(f"   Local: {conflict.get('local_path', 'unknown')}")
        print(f"   Obsidian: {conflict.get('obsidian_path', 'unknown')}")
        print("")
EOF
}

# Main function
main() {
  case "${1:-sync}" in
    sync)
      sync_all_notes
      ;;
    sync-daily-logs)
      sync_daily_logs_bidirectional
      ;;
    detect-changes)
      detect_obsidian_changes "${2:-7}"
      ;;
    status)
      show_status
      ;;
    resolve)
      resolve_conflicts
      ;;
    help|*)
      echo "Usage: gtd-brain-bidirectional-sync <command> [args]"
      echo ""
      echo "Commands:"
      echo "  sync                - Full bidirectional sync (default)"
      echo "  sync-daily-logs     - Sync only daily logs"
      echo "  detect-changes [N]  - Detect changes in Obsidian (last N days, default: 7)"
      echo "  status              - Show sync status"
      echo "  resolve             - Resolve conflicts"
      echo ""
      echo "Examples:"
      echo "  gtd-brain-bidirectional-sync                    # Full sync"
      echo "  gtd-brain-bidirectional-sync sync-daily-logs    # Sync daily logs only"
      echo "  gtd-brain-bidirectional-sync detect-changes 30  # Check last 30 days"
      echo "  gtd-brain-bidirectional-sync status             # Show status"
      echo ""
      echo "Features:"
      echo "  âœ“ Detects changes made in Obsidian (mobile/other computers)"
      echo "  âœ“ Merges conflicts intelligently"
      echo "  âœ“ Syncs metadata (tags, links, status)"
      echo "  âœ“ Tracks timestamps to identify source of truth"
      echo "  âœ“ Works even when no local daily log exists"
      ;;
  esac
}

main "$@"
